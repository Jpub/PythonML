{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colaboratory를 사용할 때는 다음 주석을 해제하고 실행하기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 다음을 실행하면 authorization code 입력을 요청받습니다.\n",
    "# # 출력된 링크를 클릭하고 Google 계정으로 로그인한 뒤,\n",
    "# # authorization code를 복사해서 붙여 넣습니다.\n",
    "# import os\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working_dir = 'MLSys_100Knocks' #　※※ 여러분이 만든 폴더 경로가 다를 때는 다음을 변경합니다. ※※\n",
    "# path = f'/content/drive/MyDrive/{working_dir}/MainChapter/chapter07'\n",
    "# os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter notebook ipywidgets 활성화\n",
    "# for jupyter notebook (virtualenv 사용 시)\n",
    "#!jupyter nbextension enable --user --py widgetsnbextension\n",
    "\n",
    "# for jupyter lab\n",
    "#!jupyter labextension install @jupyter-widgets/jupyterlab-manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7장 머신러닝 모델을 구축하는 테크닉 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테크닉 61: 폴더를 만들고 머신러닝용 데이터를 저장하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = 'data'\n",
    "input_dir = os.path.join(data_dir, '0_input')\n",
    "output_dir = os.path.join(data_dir, '1_output')\n",
    "os.makedirs(input_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **반드시 데이터를 폴더에 넣어주세요**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ml_data_file = 'ml_base_data.csv'\n",
    "ml_data = pd.read_csv(os.path.join(input_dir, ml_data_file))\n",
    "ml_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테크닉 62: 범주형 변수에 대응하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_data = pd.get_dummies(ml_data['store_name'],\n",
    "                               prefix='store', prefix_sep='_')\n",
    "display(category_data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del category_data['store_가덕해안로점']\n",
    "del ml_data['year_month']\n",
    "del ml_data['store_name']\n",
    "ml_data = pd.concat([ml_data, category_data], axis=1)\n",
    "ml_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테크닉 63: 학습 데이터와 테스트 데이터를 나누자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(ml_data, test_size=0.3, random_state=0)\n",
    "print(f'Train：{len(train_data)}건/ Test:{len(test_data)}')\n",
    "print(f'Weekday Train0：{len(train_data.loc[train_data[\"y_weekday\"] == 0])}건')\n",
    "print(f'Weekday Train1：{len(train_data.loc[train_data[\"y_weekday\"] == 1])}건')\n",
    "print(f'Weekday Test0：{len(test_data.loc[test_data[\"y_weekday\"] == 0])}건')\n",
    "print(f'Weekday Test1：{len(test_data.loc[test_data[\"y_weekday\"] == 1])}건')\n",
    "\n",
    "print(f'Weekend Train0：{len(train_data.loc[train_data[\"y_weekend\"] == 0])}건')\n",
    "print(f'Weekend Train1：{len(train_data.loc[train_data[\"y_weekend\"] == 1])}건')\n",
    "print(f'Weekend Test0：{len(test_data.loc[test_data[\"y_weekend\"] == 0])}건')\n",
    "print(f'Weekend Test1：{len(test_data.loc[test_data[\"y_weekend\"] == 1])}건')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테크닉 64: 모델 하나를 구현하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_cols = list(train_data.columns)\n",
    "X_cols.remove('y_weekday')\n",
    "X_cols.remove('y_weekend')\n",
    "target_y = 'y_weekday'\n",
    "y_train = train_data[target_y]\n",
    "X_train = train_data[X_cols]\n",
    "y_test = test_data[target_y]\n",
    "X_test = test_data[X_cols]\n",
    "display(y_train.head(3))\n",
    "display(X_train.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=0)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테크닉 65: 모델을 평가하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, \\\n",
    "    precision_score, confusion_matrix\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_pred_train)\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "f1_train = f1_score(y_train, y_pred_train)\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "recall_train = recall_score(y_train, y_pred_train)\n",
    "recall_test = recall_score(y_test, y_pred_test)\n",
    "precision_train = precision_score(y_train, y_pred_train)\n",
    "precision_test = precision_score(y_test, y_pred_test)\n",
    "print(f'[정답률] Train：{round(acc_train, 2)} '\n",
    "      f'Test：{round(acc_test, 2)}')\n",
    "print(f'[F값] Train：{round(f1_train, 2)} '\n",
    "      f'Test：{round(f1_test, 2)}')\n",
    "print(f'[재현율] Train：{round(recall_train, 2)} '\n",
    "      f'Test：{round(recall_test, 2)}')\n",
    "print(f'[적합률] Train：{round(precision_train, 2)} '\n",
    "      f'Test：{round(precision_test, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_train, y_pred_train))\n",
    "print(confusion_matrix(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_train, fp_train, fn_train, tp_train = \\\n",
    "    confusion_matrix(y_train, y_pred_train).ravel()\n",
    "tn_test, fp_test, fn_test, tp_test = \\\n",
    "    confusion_matrix(y_test, y_pred_test).ravel()\n",
    "print(f'[혼동 행렬] Train：{tn_train}, {fp_train}, {fn_train}, {tp_train}')\n",
    "print(f'[혼동 행렬] Test：{tn_test}, {fp_test}, {fn_test}, {tp_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_train = pd.DataFrame({\n",
    "    'DataCategory': ['train'], 'acc': [acc_train], 'f1': [f1_train],\n",
    "    'recall': [recall_train], 'precision': [precision_train],\n",
    "    'tp': [tp_train], 'fn': [fn_train], 'fp': [fp_train], 'tn': [tn_train]})\n",
    "score_test = pd.DataFrame({\n",
    "    'DataCategory': ['test'], 'acc': [acc_test], 'f1': [f1_test],\n",
    "    'recall': [recall_test], 'precision': [precision_test],\n",
    "    'tp': [tp_test], 'fn': [fn_test], 'fp': [fp_test], 'tn': [tn_test]})\n",
    "score = pd.concat([score_train, score_test], ignore_index=True)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테크닉 66: 모델의 중요도를 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.DataFrame({\n",
    "    'cols': X_train.columns, 'importance': model.feature_importances_})\n",
    "importance = importance.sort_values('importance', ascending=False)\n",
    "importance.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테크닉 67: 모델 구현부터 평가까지의 과정을 함수화하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_and_eval(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    acc_train = accuracy_score(y_train, y_pred_train)\n",
    "    acc_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "    f1_train = f1_score(y_train, y_pred_train)\n",
    "    f1_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "    recall_train = recall_score(y_train, y_pred_train)\n",
    "    recall_test = recall_score(y_test, y_pred_test)\n",
    "\n",
    "    precision_train = precision_score(y_train, y_pred_train)\n",
    "    precision_test = precision_score(y_test, y_pred_test)\n",
    "\n",
    "    tn_train, fp_train, fn_train, tp_train = \\\n",
    "        confusion_matrix(y_train, y_pred_train).ravel()\n",
    "    tn_test, fp_test, fn_test, tp_test = \\\n",
    "        confusion_matrix(y_test, y_pred_test).ravel()\n",
    "\n",
    "    score_train = pd.DataFrame({\n",
    "        'DataCategory': ['train'], 'acc': [acc_train], 'f1': [f1_train],\n",
    "        'recall': [recall_train], 'precision': [precision_train],\n",
    "        'tp': [tp_train], 'fn': [fn_train], 'fp': [fp_train], 'tn': [tn_train]})\n",
    "    score_test = pd.DataFrame({\n",
    "        'DataCategory': ['test'], 'acc': [acc_test], 'f1': [f1_test],\n",
    "        'recall': [recall_test], 'precision': [precision_test],\n",
    "        'tp': [tp_test], 'fn': [fn_test], 'fp': [fp_test], 'tn': [tn_test]})\n",
    "\n",
    "    score = pd.concat([score_train, score_test], ignore_index=True)\n",
    "    importance = pd.DataFrame({\n",
    "        'cols': X_train.columns, 'importance': model.feature_importances_})\n",
    "    importance = importance.sort_values('importance', ascending=False)\n",
    "    cols = pd.DataFrame({'X_cols': X_train.columns})\n",
    "    display(score)\n",
    "    return score, importance, model, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(random_state=0)\n",
    "score, importance, model, cols = make_model_and_eval(\n",
    "    model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테크닉 68: 모델 파일과 평가 결과를 출력하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "target_output_dir_name = 'results_' + now\n",
    "target_output_dir = os.path.join(output_dir, target_output_dir_name)\n",
    "os.makedirs(target_output_dir, exist_ok=True)\n",
    "print(target_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_name = 'score.csv'\n",
    "importance_name = 'importance.csv'\n",
    "cols_name = 'X_cols.csv'\n",
    "model_nema = 'model.pickle'\n",
    "score_path = os.path.join(target_output_dir, score_name)\n",
    "importance_path = os.path.join(target_output_dir, importance_name)\n",
    "cols_path = os.path.join(target_output_dir, cols_name)\n",
    "model_path = os.path.join(target_output_dir, model_nema)\n",
    "\n",
    "score.to_csv(score_path, index=False)\n",
    "importance.to_csv(importance_path, index=False)\n",
    "cols.to_csv(cols_path, index=False)\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(model_path, mode='wb') as f:\n",
    "    pickle.dump(model, f, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테크닉 69: 알고리즘을 확장해 다각적으로 평가하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble \\\n",
    "    import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "models = {'tree': DecisionTreeClassifier(random_state=0),\n",
    "          'RandomForest': RandomForestClassifier(random_state=0),\n",
    "          'GradientBoostingClassifier': \n",
    "              GradientBoostingClassifier(random_state=0)}\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "target_output_dir_name = 'results_' + now\n",
    "target_output_dir = os.path.join(output_dir, target_output_dir_name)\n",
    "os.makedirs(target_output_dir, exist_ok=True)\n",
    "print(target_output_dir)\n",
    "score_all = []\n",
    "importance_all = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(model_name)\n",
    "    score, importance, model, cols = make_model_and_eval(\n",
    "        model, X_train, X_test, y_train, y_test)\n",
    "    score['model_name'] = model_name\n",
    "    importance['model_name'] = model_name\n",
    "\n",
    "    model_nema = f'model_{model_name}.pickle'\n",
    "    model_path = os.path.join(target_output_dir, model_nema)\n",
    "    with open(model_path, mode='wb') as f:\n",
    "        pickle.dump(model, f, protocol=2)\n",
    "    score_all.append(score)\n",
    "    importance_all.append(importance)\n",
    "\n",
    "score_all = pd.concat(score_all, ignore_index=True)\n",
    "importance_all = pd.concat(importance_all, ignore_index=True)\n",
    "cols = pd.DataFrame({'X_cols': X_train.columns})\n",
    "score_name = 'score.csv'\n",
    "importance_name = 'importance.csv'\n",
    "cols_name = 'X_cols.csv'\n",
    "score_path = os.path.join(target_output_dir, score_name)\n",
    "importance_path = os.path.join(target_output_dir, importance_name)\n",
    "cols_path = os.path.join(target_output_dir, cols_name)\n",
    "score_all.to_csv(score_path, index=False)\n",
    "importance_all.to_csv(importance_path, index=False)\n",
    "cols.to_csv(cols_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테크닉 70: 평일/휴일 모델을 한 번에 실행하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols = list(train_data.columns)\n",
    "X_cols.remove('y_weekday')\n",
    "X_cols.remove('y_weekend')\n",
    "targets_y = ['y_weekday', 'y_weekend']\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "target_output_dir_name = 'results_' + now\n",
    "target_output_dir = os.path.join(output_dir, target_output_dir_name)\n",
    "os.makedirs(target_output_dir, exist_ok=True)\n",
    "print(target_output_dir)\n",
    "\n",
    "score_all = []\n",
    "importance_all = []\n",
    "\n",
    "for target_y in targets_y:\n",
    "    y_train = train_data[target_y]\n",
    "    X_train = train_data[X_cols]\n",
    "    y_test = test_data[target_y]\n",
    "    X_test = test_data[X_cols]\n",
    "\n",
    "    models = {'tree': DecisionTreeClassifier(random_state=0),\n",
    "              'RandomForest': RandomForestClassifier(random_state=0),\n",
    "              'GradientBoosting': GradientBoostingClassifier(random_state=0)}\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(model_name)\n",
    "        score, importance, model, cols = make_model_and_eval(\n",
    "            model, X_train, X_test, y_train, y_test)\n",
    "        score['model_name'] = model_name\n",
    "        importance['model_name'] = model_name\n",
    "        score['model_target'] = target_y\n",
    "        importance['model_target'] = target_y\n",
    "\n",
    "        model_nema = f'model_{target_y}_{model_name}.pickle'\n",
    "        model_path = os.path.join(target_output_dir, model_nema)\n",
    "        with open(model_path, mode='wb') as f:\n",
    "            pickle.dump(model, f, protocol=2)\n",
    "        score_all.append(score)\n",
    "        importance_all.append(importance)\n",
    "\n",
    "score_all = pd.concat(score_all, ignore_index=True)\n",
    "importance_all = pd.concat(importance_all, ignore_index=True)\n",
    "cols = pd.DataFrame({'X_cols': X_train.columns})\n",
    "\n",
    "score_name = 'score.csv'\n",
    "importance_name = 'importance.csv'\n",
    "cols_name = 'X_cols.csv'\n",
    "score_path = os.path.join(target_output_dir, score_name)\n",
    "importance_path = os.path.join(target_output_dir, importance_name)\n",
    "cols_path = os.path.join(target_output_dir, cols_name)\n",
    "score_all.to_csv(score_path, index=False)\n",
    "importance_all.to_csv(importance_path, index=False)\n",
    "cols.to_csv(cols_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_all.loc[score_all['model_target']=='y_weekday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_all.loc[score_all['model_target']=='y_weekend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_all.loc[\n",
    "    (importance_all['model_target'] == 'y_weekday') &\n",
    "    (importance_all['model_name'] == 'GradientBoosting')].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
