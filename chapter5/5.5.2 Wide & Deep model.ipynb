{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class WideAndDeepModel:\n",
    "    def __init__(self, wide_length, deep_length, deep_last_layer_len, softmax_label):\n",
    "        self.input_wide_part = tf.placeholder(tf.float32, shape=[None, wide_length], name='input_wide_part')\n",
    "        self.input_deep_part = tf.placeholder(tf.float32, shape=[None, deep_length], name='input_deep_part')\n",
    "        self.input_y = tf.placeholder(tf.float32, shape=[None, softmax_label], name='input_y')\n",
    "\n",
    "        with tf.name_scope('deep_part'):\n",
    "            w_x1 = tf.Variable(tf.random_normal([wide_length, 64], stddev=0.03), name='w_x1')\n",
    "            b_x1 = tf.Variable(tf.random_normal([64]), name='b_x1')\n",
    "\n",
    "            w_x2 = tf.Variable(tf.random_normal([64, deep_last_layer_len], stddev=0.03), name='w_x2')\n",
    "            b_x2 = tf.Variable(tf.random_normal([deep_last_layer_len]), name='b_x2')\n",
    "\n",
    "            z1 = tf.add(tf.matmul(self.input_wide_part, w_x1), b_x1)\n",
    "            a1 = tf.nn.relu(z1)\n",
    "            self.deep_logits = tf.add(tf.matmul(a1, w_x2), b_x2)\n",
    "\n",
    "        with tf.name_scope('wide_part'):\n",
    "            weights = tf.Variable(tf.truncated_normal([deep_last_layer_len + wide_length, softmax_label]))\n",
    "            biases = tf.Variable(tf.zeros([softmax_label]))\n",
    "\n",
    "            self.wide_and_deep = tf.concat([self.deep_logits, self.input_wide_part], axis = 1)\n",
    "\n",
    "            self.wide_and_deep_logits = tf.add(tf.matmul(self.wide_and_deep, weights), biases)\n",
    "            self.predictions = tf.argmax(self.wide_and_deep_logits, 1, name= \"prediction\")\n",
    "\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.wide_and_deep_logits, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, axis=1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터와 레이블 읽어오기\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "def load_data_and_labels(path):\n",
    "    data = []\n",
    "    y = []\n",
    "    total_q = []\n",
    "\n",
    "    # count = 0\n",
    "    with open(path, 'r') as f:\n",
    "        rdr = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "        for row in rdr:\n",
    "            y.append(float(row[1]))\n",
    "\n",
    "\n",
    "    # data = np.asarray(data)\n",
    "    total_q = np.asarray(total_q)\n",
    "    y = np.asarray(y)\n",
    "    return data, y\n",
    "\n",
    "\n",
    "data, y = load_data_and_labels('../data/zutao2.csv')\n",
    "\n",
    "bins = pd.qcut(y, 50, retbins=True)\n",
    "print(bins[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import data_helpers\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "from WideandDeepModel import WideAndDeepModel\n",
    "\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_string(\"train_dir\", \"../data/cvr_train_data.csv\", \"Path of train data\")\n",
    "tf.flags.DEFINE_integer(\"wide_length\", 261, \"Path of train data\")\n",
    "tf.flags.DEFINE_integer(\"deep_length\", 261, \"Path of train data\")\n",
    "tf.flags.DEFINE_integer(\"deep_last_layer_len\", 32, \"Path of train data\")\n",
    "tf.flags.DEFINE_integer(\"softmax_label\", 1, \"Path of train data\")\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 32, \"Batch Size\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 30, \"Number of training epochs\")\n",
    "tf.flags.DEFINE_integer(\"display_every\", 50, \"Number of iterations to display training info.\")\n",
    "tf.flags.DEFINE_float(\"learning_rate\", 1e-3, \"Which learning rate to start with.\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 200, \"Save model after this many steps\")\n",
    "\n",
    "\n",
    "\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "\n",
    "def train():\n",
    "    with tf.device('/cpu:0'):\n",
    "        x, y = data_helpers.load_data_and_labels(FLAGS.train_dir)\n",
    "\n",
    "    print('-' * 120)\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print('-' * 120)\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "            allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "            log_device_placement=FLAGS.log_device_placement)\n",
    "\n",
    "        sess = tf.Session(config=session_conf)\n",
    "\n",
    "        with sess.as_default():\n",
    "            model = WideAndDeepModel(\n",
    "                wide_length=FLAGS.wide_length,\n",
    "                deep_length=FLAGS.deep_length,\n",
    "                deep_last_layer_len=FLAGS.deep_last_layer_len,\n",
    "                softmax_label=FLAGS.softmax_label\n",
    "            )\n",
    "\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            train_op = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(model.loss, global_step=global_step)\n",
    "\n",
    "            # Output directory for models and summaries\n",
    "            # timestamp = str(int(time.time()))\n",
    "            # out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "            #\n",
    "            # checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_dir = '/Users/asukapan/workspace/all_codes/iscp_all_codes/src/wide_and_deep_for_cvr/src/model/'\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # Generate batches\n",
    "            batches = data_helpers.batch_iter(\n",
    "                list(zip(x, y)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "            # Training loop. For each batch...\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch = zip(*batch)\n",
    "\n",
    "                feed_dict = {\n",
    "                    model.input_wide_part: x_batch,\n",
    "                    model.input_deep_part: x_batch,\n",
    "                    model.input_y: y_batch\n",
    "                }\n",
    "\n",
    "                _, step, loss, accuracy = sess.run(\n",
    "                    [train_op, global_step, model.loss, model.accuracy], feed_dict)\n",
    "\n",
    "                if step % FLAGS.display_every == 0:\n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}: step {}, loss {:g}, auc {:G}\".format(time_str, step, loss, accuracy))\n",
    "\n",
    "                # Model checkpoint\n",
    "                if step % FLAGS.checkpoint_every == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "\n",
    "        save_path = saver.save(sess, checkpoint_prefix)\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    train()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
