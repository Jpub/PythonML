{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.core import Flatten, Dense, Activation, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.set_image_dim_ordering('th')\n",
    "\n",
    "def get_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Layer 1\n",
    "    model.add(Convolution2D(16, 3, 3,input_shape=(3,448,448),border_mode='same',subsample=(1,1)))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # Layer 2\n",
    "    model.add(Convolution2D(32,3,3 ,border_mode='same'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),border_mode='valid'))\n",
    "    \n",
    "    # Layer 3\n",
    "    model.add(Convolution2D(64,3,3 ,border_mode='same'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),border_mode='valid'))\n",
    "    \n",
    "    # Layer 4\n",
    "    model.add(Convolution2D(128,3,3 ,border_mode='same'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),border_mode='valid'))\n",
    "    \n",
    "    # Layer 5\n",
    "    model.add(Convolution2D(256,3,3 ,border_mode='same'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),border_mode='valid'))\n",
    "    \n",
    "    # Layer 6\n",
    "    model.add(Convolution2D(512,3,3 ,border_mode='same'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),border_mode='valid'))\n",
    "    \n",
    "    # Layer 7\n",
    "    model.add(Convolution2D(1024,3,3 ,border_mode='same'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    \n",
    "    # Layer 8\n",
    "    model.add(Convolution2D(1024,3,3 ,border_mode='same'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    \n",
    "    # Layer 9\n",
    "    model.add(Convolution2D(1024,3,3 ,border_mode='same'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Layer 10\n",
    "    model.add(Dense(256))\n",
    "    \n",
    "    # Layer 11\n",
    "    model.add(Dense(4096))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    \n",
    "    # Layer 12\n",
    "    model.add(Dense(1470))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def crop_and_resize(image):\n",
    "    cropped = image[300:650,500:,:]\n",
    "    return cv2.resize(cropped, (448,448))\n",
    "\n",
    "def normalize(image):\n",
    "    normalized = 2.0*image/255.0 - 1\n",
    "    return normalized\n",
    "\n",
    "def preprocess(image):\n",
    "    cropped = crop_and_resize(image)\n",
    "    normalized = normalize(cropped)\n",
    "    # The model works on (channel, height, width) ordering of dimensions\n",
    "    transposed = np.transpose(normalized, (2,0,1))\n",
    "    return transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = mpimg.imread('yolo.png')\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code based on:\n",
    "# YAD2K https://github.com/allanzelener/YAD2K\n",
    "# darkflow https://github.com/thtrieu/darkflow\n",
    "# Darknet.keras https://github.com/sunshineatnoon/Darknet.keras\n",
    "# https://github.com/xslittlegrass/CarND-Vehicle-Detection\n",
    "\n",
    "# Box util methods\n",
    "\n",
    "class Box:\n",
    "    def __init__(self):\n",
    "        self.x, self.y = float(), float()\n",
    "        self.w, self.h = float(), float()\n",
    "        self.c = float()\n",
    "        self.prob = float()\n",
    "        \n",
    "def overlap(x1, w1, x2, w2):\n",
    "    l1 = x1 - w1 / 2.\n",
    "    l2 = x2 - w2 / 2.\n",
    "    left = max(l1, l2)\n",
    "    r1 = x1 + w1 / 2.\n",
    "    r2 = x2 + w2 / 2.\n",
    "    right = min(r1, r2)\n",
    "    return right - left\n",
    "\n",
    "\n",
    "def box_intersection(a, b):\n",
    "    \"\"\"\n",
    "\n",
    "    :param a: Box 1\n",
    "    :param b: Box 2\n",
    "    :return: Intersection area of the 2 boxes\n",
    "    \"\"\"\n",
    "    w = overlap(a.x, a.w, b.x, b.w)\n",
    "    h = overlap(a.y, a.h, b.y, b.h)\n",
    "    if w < 0 or h < 0:\n",
    "        return 0\n",
    "    area = w * h\n",
    "    return area\n",
    "\n",
    "\n",
    "def box_union(a, b):\n",
    "    \"\"\"\n",
    "\n",
    "    :param a: Box 1\n",
    "    :param b: Box 2\n",
    "    :return: Area under the union of the 2 boxes\n",
    "    \"\"\"\n",
    "    i = box_intersection(a, b)\n",
    "    u = a.w * a.h + b.w * b.h - i\n",
    "    return u\n",
    "\n",
    "\n",
    "def box_iou(a, b):\n",
    "    \"\"\"\n",
    "\n",
    "    :param a: Box 1\n",
    "    :param b: Box 2\n",
    "    :return: Intersection over union, which is ratio of intersection area to union area of the 2 boxes\n",
    "    \"\"\"\n",
    "    return box_intersection(a, b) / box_union(a, b)\n",
    "\n",
    "\n",
    "\n",
    "def yolo_output_to_car_boxes(yolo_output, threshold=0.2, sqrt=1.8, C=20, B=2, S=7):\n",
    "\n",
    "    # Position for class 'car' in the VOC dataset classes\n",
    "    car_class_number = 6\n",
    "\n",
    "    boxes = []\n",
    "    SS = S*S  # number of grid cells\n",
    "    prob_size = SS*C  # class probabilities\n",
    "    conf_size = SS*B  # confidences for each grid cell\n",
    "\n",
    "    probabilities = yolo_output[0:prob_size]\n",
    "    confidence_scores = yolo_output[prob_size: (prob_size + conf_size)]\n",
    "    cords = yolo_output[(prob_size + conf_size):]\n",
    "\n",
    "    # Reshape the arrays so that its easier to loop over them\n",
    "    probabilities = probabilities.reshape((SS, C))\n",
    "    confs = confidence_scores.reshape((SS, B))\n",
    "    cords = cords.reshape((SS, B, 4))\n",
    "\n",
    "    for grid in range(SS):\n",
    "        for b in range(B):\n",
    "            bx = Box()\n",
    "\n",
    "            bx.c = confs[grid, b]\n",
    "\n",
    "            # bounding box xand y coordinates are offsets of a particular grid cell location,\n",
    "            # so they are also bounded between 0 and 1.\n",
    "            # convert them absolute locations relative to the image size\n",
    "            bx.x = (cords[grid, b, 0] + grid % S) / S\n",
    "            bx.y = (cords[grid, b, 1] + grid // S) / S\n",
    "\n",
    "\n",
    "            bx.w = cords[grid, b, 2] ** sqrt\n",
    "            bx.h = cords[grid, b, 3] ** sqrt\n",
    "\n",
    "            # multiply confidence scores with class probabilities to get class sepcific confidence scores\n",
    "            p = probabilities[grid, :] * bx.c\n",
    "\n",
    "            # Check if the confidence score for class 'car' is greater than the threshold\n",
    "            if p[car_class_number] >= threshold:\n",
    "                bx.prob = p[car_class_number]\n",
    "                boxes.append(bx)\n",
    "\n",
    "    # combine boxes that are overlap\n",
    "\n",
    "    # sort the boxes by confidence score, in the descending order\n",
    "    boxes.sort(key=lambda b: b.prob, reverse=True)\n",
    "\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        boxi = boxes[i]\n",
    "        if boxi.prob == 0:\n",
    "            continue\n",
    "\n",
    "        for j in range(i + 1, len(boxes)):\n",
    "            boxj = boxes[j]\n",
    "\n",
    "            # If boxes have more than 40% overlap then retain the box with the highest confidence score\n",
    "            if box_iou(boxi, boxj) >= 0.4:\n",
    "                boxes[j].prob = 0\n",
    "\n",
    "    boxes = [b for b in boxes if b.prob > 0]\n",
    "\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def draw_boxes(boxes,im, crop_dim):\n",
    "    imgcv1 = im.copy()\n",
    "    [xmin, xmax] = crop_dim[0]\n",
    "    [ymin, ymax] = crop_dim[1]\n",
    "    \n",
    "    height, width, _ = imgcv1.shape\n",
    "    for b in boxes:\n",
    "        w = xmax - xmin\n",
    "        h = ymax - ymin\n",
    "\n",
    "        left  = int ((b.x - b.w/2.) * w) + xmin\n",
    "        right = int ((b.x + b.w/2.) * w) + xmin\n",
    "        top   = int ((b.y - b.h/2.) * h) + ymin\n",
    "        bot   = int ((b.y + b.h/2.) * h) + ymin\n",
    "\n",
    "        if left  < 0:\n",
    "            left = 0\n",
    "        if right > width - 1:\n",
    "            right = width - 1\n",
    "        if top < 0:\n",
    "            top = 0\n",
    "        if bot>height - 1: \n",
    "            bot = height - 1\n",
    "        \n",
    "        thick = 5 #int((height + width // 150))\n",
    "        \n",
    "        cv2.rectangle(imgcv1, (left, top), (right, bot), (255,0,0), thick)\n",
    "\n",
    "    return imgcv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load weights (p.224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights\n",
    "from utils import load_weights\n",
    "\n",
    "model = get_model()\n",
    "load_weights(model,'yolo-tiny.weights')\n",
    "\n",
    "test_image = mpimg.imread('test_images/test1.jpg')\n",
    "pre_processed = preprocess(test_image)\n",
    "batch = np.expand_dims(pre_processed, axis=0)\n",
    "batch_output = model.predict(batch)\n",
    "print(batch_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ":\n",
    "boxes = yolo_output_to_car_boxes(batch_output[0], threshold=0.25)\n",
    "final = draw_boxes(boxes, test_image, ((500,1280),(300,650)))\n",
    "\n",
    "# plt.rcParams['figure.figsize'] = (10, 5.6)\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_image)\n",
    "plt.axis('off')\n",
    "plt.title(\"Original Image\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(final)\n",
    "plt.axis('off')\n",
    "plt.title(\"With Boxes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final pipeline\n",
    "def pipeline(image):\n",
    "    pre_processed = preprocess(image)\n",
    "    batch = np.expand_dims(pre_processed, axis=0)\n",
    "    batch_output = model.predict(batch)\n",
    "    boxes = yolo_output_to_car_boxes(batch_output[0], threshold=0.20)\n",
    "    final = draw_boxes(boxes, image, ((500,1280),(300,650)))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filenames = glob.glob(\"test_images/*.jpg\")\n",
    "num_files = len(filenames)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 20)\n",
    "\n",
    "for i in range(num_files):\n",
    "    image = mpimg.imread(filenames[i])\n",
    "    final = pipeline(image)\n",
    "    mpimg.imsave(\"output_images/test%d.jpg\" % (i+1), final)\n",
    "    \n",
    "    plt.subplot(num_files,2,i*2+1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Test Image %d\" % (i+1))\n",
    "    plt.subplot(num_files,2,i*2+2)\n",
    "    plt.imshow(final)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply it on a video\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "project_video_output = 'project_video_output.mp4'\n",
    "clip1 = VideoFileClip(\"project_video.mp4\")\n",
    "lane_clip = clip1.fl_image(pipeline)\n",
    "%time lane_clip.write_videofile(project_video_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
